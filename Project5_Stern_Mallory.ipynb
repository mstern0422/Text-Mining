{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d233ac50",
   "metadata": {},
   "source": [
    "### 1. Load the dataset of Twitter_Data.csv into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a3b207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kiya tho refresh maarkefir comment karo</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>surat women perform yagna seeks divine grace f...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this comes from cabinet which has scholars lik...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>with upcoming election india saga going import...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gandhi was gay does modi</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0\n",
       "5           kiya tho refresh maarkefir comment karo        0.0\n",
       "6  surat women perform yagna seeks divine grace f...       0.0\n",
       "7  this comes from cabinet which has scholars lik...       0.0\n",
       "8  with upcoming election india saga going import...       1.0\n",
       "9                         gandhi was gay does modi         1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from scipy.sparse.linalg import norm \n",
    "twitter_data = pd.read_csv('/Users/mj/Desktop/DSCI 314 (Text Mining)/Project4Data/Twitter_Data.csv', sep = ',')\n",
    "twitter_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7569a4db",
   "metadata": {},
   "source": [
    "### 2. Find the cosine similarity in clean_text between the 100th and 1,000th tweets using dot and norm function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d85d74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the 100th and 1000th lines from twitter_data is [[0.06056186]]\n"
     ]
    }
   ],
   "source": [
    "# First reate a TfidfVectorizer object\n",
    "tfidfvectorizer = TfidfVectorizer(use_idf = True,smooth_idf = True, sublinear_tf = False)\n",
    "\n",
    "# Fit 100th and 1000th lines into matrix\n",
    "tfidfmatrix = tfidfvectorizer.fit_transform(twitter_data['clean_text'].iloc[[99, 999]].astype('U'))\n",
    "\n",
    "# Compute and print the cosine similarity of the two lines of text using dot and norm\n",
    "cosim1 = dot(tfidfmatrix[0, :], tfidfmatrix[1, :].T)/(norm(tfidfmatrix[0, :]) * norm(tfidfmatrix[1, :]))\n",
    "print(f\"The cosine similarity between the 100th and 1000th lines from twitter_data is {cosim1.todense()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767afa02",
   "metadata": {},
   "source": [
    "### 3. Find the cosine similarity in clean_text between the 100th and 1,000th tweets using cosine_similarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b976824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the 100th and 1000th lines from twitter_data is 0.06056186021915369\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# The same TfidfVectorizer object and matrix will be used from the previous cell\n",
    "# Compute and print the cosine similarity of the two lines of text using cosine_similarity function\n",
    "cosim2 = cosine_similarity(tfidfmatrix, dense_output = True)\n",
    "print(f\"The cosine similarity between the 100th and 1000th lines from twitter_data is {cosim2[0,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ce843",
   "metadata": {},
   "source": [
    "### 4. Generate a sparse matrix using cosine_similarity for the first 100th tweets in this dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef68c2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 33)\t0.0580062561251775\n",
      "  (0, 68)\t0.04328296903058072\n",
      "  (0, 80)\t0.12478581358882516\n",
      "  (0, 43)\t0.04605543445083791\n",
      "  (0, 92)\t0.02110453149853594\n",
      "  (0, 61)\t0.0215382138166492\n",
      "  (0, 52)\t0.06594059038722079\n",
      "  (0, 50)\t0.10233104028117342\n",
      "  (0, 44)\t0.019984940080632955\n",
      "  (0, 37)\t0.08211683174901775\n",
      "  (0, 31)\t0.0461795075257483\n",
      "  (0, 29)\t0.018573103968754445\n",
      "  (0, 17)\t0.049621636402247524\n",
      "  (0, 14)\t0.016566600345139352\n",
      "  (0, 13)\t0.015577973399905637\n",
      "  (0, 98)\t0.11681016673698208\n",
      "  (0, 99)\t0.010554104470027607\n",
      "  (0, 97)\t0.0044580782125296994\n",
      "  (0, 94)\t0.005497584097850677\n",
      "  (0, 90)\t0.03875239059542715\n",
      "  (0, 89)\t0.036896419643522485\n",
      "  (0, 88)\t0.026093621079010917\n",
      "  (0, 87)\t0.03066649793537978\n",
      "  (0, 86)\t0.06860290714228434\n",
      "  (0, 85)\t0.06494517491078422\n",
      "  :\t:\n",
      "  (99, 30)\t0.04463159444397126\n",
      "  (99, 28)\t0.0066373881454612265\n",
      "  (99, 27)\t0.004123307171274124\n",
      "  (99, 26)\t0.027902469041352265\n",
      "  (99, 25)\t0.050764692189991606\n",
      "  (99, 24)\t0.014437266025126544\n",
      "  (99, 23)\t0.02549676558053407\n",
      "  (99, 21)\t0.02916430483633934\n",
      "  (99, 20)\t0.01175640740680816\n",
      "  (99, 19)\t0.014482034473794196\n",
      "  (99, 18)\t0.0017015770266885588\n",
      "  (99, 16)\t0.030086346623025728\n",
      "  (99, 15)\t0.002570971160106238\n",
      "  (99, 12)\t0.004762510606394496\n",
      "  (99, 11)\t0.03574817335686442\n",
      "  (99, 10)\t0.009603814622673068\n",
      "  (99, 9)\t0.005425730203453846\n",
      "  (99, 8)\t0.029784909950526718\n",
      "  (99, 7)\t0.0032440830365396763\n",
      "  (99, 6)\t0.0032461204844582156\n",
      "  (99, 4)\t0.003045206136988965\n",
      "  (99, 3)\t0.010041147663010284\n",
      "  (99, 2)\t0.008024977438742899\n",
      "  (99, 1)\t0.0036745295666812225\n",
      "  (99, 0)\t0.010554104470027607\n"
     ]
    }
   ],
   "source": [
    "# The same TfidfVectorizer object will be used from the previous cell\n",
    "# Fit first 100 lines from twitter_data to matrix\n",
    "tfidfmatrix100 = tfidfvectorizer.fit_transform(twitter_data['clean_text'].head(100).astype('U'))\n",
    "\n",
    "# Creating object from sparse amtrix using cosine_similarity function then printing\n",
    "cosimmatrix = cosine_similarity(tfidfmatrix100, dense_output = False)\n",
    "print(cosimmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f117d6c",
   "metadata": {},
   "source": [
    "### 5. Find the highest cosine similarity for the first 100th tweets in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e5bdece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The similarity between [when modi promised “minimum government maximum governance” expected him begin the difficult job reforming the state why does take years get justice state should and not business and should exit psus and temples] and [being born religion where female deities worshipped its misogynistic sadistic tradition and totally against point isits man made tradition and not written one not religious lunatic support own religion its repressive] is 0.06\n"
     ]
    }
   ],
   "source": [
    "# Convert matrix from previous cell to COOrdinate format\n",
    "SMcoo = cosimmatrix.tocoo()\n",
    "\n",
    "# Sort the data\n",
    "sortedsm = np.argsort(SMcoo.data)\n",
    "\n",
    "# Get the rows and cols\n",
    "rows = SMcoo.row\n",
    "colums = SMcoo.col\n",
    "\n",
    "print(f\" The similarity between [{twitter_data['clean_text'][rows[0]]}] and [{twitter_data['clean_text'][colums[0]]}] is {SMcoo.data[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd7141",
   "metadata": {},
   "source": [
    "### 6. Find the top 3 words that are closest to 'France - Paris + Berlin' using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6db9fcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/mj/Desktop/GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the pretrained Word2vec model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m KeyedVectors\u001b[38;5;241m.\u001b[39mload_word2vec_format(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/mj/Desktop/GoogleNews-vectors-negative300.bin\u001b[39m\u001b[38;5;124m'\u001b[39m, binary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Find the word 'France - Paris + Berlin' is closest to:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmost_similar(positive\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mberlin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrance\u001b[39m\u001b[38;5;124m'\u001b[39m], negative\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparis\u001b[39m\u001b[38;5;124m'\u001b[39m], topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1720\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39mfvocab, binary\u001b[38;5;241m=\u001b[39mbinary, encoding\u001b[38;5;241m=\u001b[39mencoding, unicode_errors\u001b[38;5;241m=\u001b[39municode_errors,\n\u001b[1;32m   1721\u001b[0m         limit\u001b[38;5;241m=\u001b[39mlimit, datatype\u001b[38;5;241m=\u001b[39mdatatype, no_header\u001b[38;5;241m=\u001b[39mno_header,\n\u001b[1;32m   1722\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2045\u001b[0m             counts[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(count)\n\u001b[1;32m   2047\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading projection weights from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[0;32m-> 2048\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_header:\n\u001b[1;32m   2050\u001b[0m         \u001b[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/smart_open/smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 188\u001b[0m fobj \u001b[38;5;241m=\u001b[39m _shortcut_open(\n\u001b[1;32m    189\u001b[0m     uri,\n\u001b[1;32m    190\u001b[0m     mode,\n\u001b[1;32m    191\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    192\u001b[0m     buffering\u001b[38;5;241m=\u001b[39mbuffering,\n\u001b[1;32m    193\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    194\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    195\u001b[0m     newline\u001b[38;5;241m=\u001b[39mnewline,\n\u001b[1;32m    196\u001b[0m )\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/smart_open/smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    359\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[38;5;241m=\u001b[39mbuffering, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mj/Desktop/GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors \n",
    "\n",
    "# Load the pretrained Word2vec model\n",
    "model = KeyedVectors.load_word2vec_format('/Users/mj/Desktop/GoogleNews-vectors-negative300.bin', binary = True)\n",
    "\n",
    "# Find the word 'France - Paris + Berlin' is closest to:\n",
    "result = model.most_similar(positive=['berlin', 'france'], negative=['paris'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0aa374",
   "metadata": {},
   "source": [
    "### 7. Compute the document similarity (Word Mover’s Distance) between the 100th and 1,000th tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from pyemd import emd\n",
    "\n",
    "# Using same model from previous cell\n",
    "# Find wmd between 100th 1000th tweet\n",
    "wmd100 = model.wmdistance(twitter_data['clean_text'].iloc[[99]].astype('U'), twitter_data['clean_text'].iloc[[999]].astype('U'))\n",
    "wmd100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e60c6",
   "metadata": {},
   "source": [
    "### 8. Remove stopwords before computing the document similarity (Word Mover’s Distance) between the 100th and 1,000th tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0982e81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords')  \n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Function to remove stop words from given tweets\n",
    "def preprocess(tweet):\n",
    "    return [w for w in tweet.str.lower().str.split() if w not in stop_words]\n",
    "\n",
    "# Removing stop words\n",
    "tweet100 = preprocess(twitter_data['clean_text'].iloc[[99]].astype('U'))\n",
    "tweet1000 = preprocess(twitter_data['clean_text'].iloc[[999]].astype('U'))\n",
    "\n",
    "# Recomputing WMD from 100th and 1000th tweet\n",
    "wmd1000 = model.wmdistance(tweet100, tweet1000)\n",
    "wmd1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47cc0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
