{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add820d8",
   "metadata": {},
   "source": [
    "### 1. Load the dataset of sentiment140.csv into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0cee00e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>968114</th>\n",
       "      <td>4</td>\n",
       "      <td>1827836862</td>\n",
       "      <td>Sun May 17 11:32:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>uk_cam</td>\n",
       "      <td>@Claire_Cordon gender is just one way that lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71229</th>\n",
       "      <td>0</td>\n",
       "      <td>1693896470</td>\n",
       "      <td>Mon May 04 00:12:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>caaaaaity</td>\n",
       "      <td>missed the train , that tends to happen to me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646698</th>\n",
       "      <td>0</td>\n",
       "      <td>2236575372</td>\n",
       "      <td>Fri Jun 19 04:02:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>littleemilyjane</td>\n",
       "      <td>off school sick  mum refused to let me go to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011093</th>\n",
       "      <td>4</td>\n",
       "      <td>1881066754</td>\n",
       "      <td>Fri May 22 03:29:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ClareOz</td>\n",
       "      <td>whaat a loaad a rubbish  @mpisthename howsit? x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321808</th>\n",
       "      <td>0</td>\n",
       "      <td>2003928077</td>\n",
       "      <td>Tue Jun 02 07:23:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LolaOnTwatter</td>\n",
       "      <td>i wish i could eat a huge bowl of capn crunch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201765</th>\n",
       "      <td>4</td>\n",
       "      <td>1985734317</td>\n",
       "      <td>Sun May 31 17:40:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>wisdompathart</td>\n",
       "      <td>@Marge_Inovera awww. sending you some hugs and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345043</th>\n",
       "      <td>4</td>\n",
       "      <td>2043910645</td>\n",
       "      <td>Fri Jun 05 08:35:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>vickyjones91</td>\n",
       "      <td>i hate buses. so need to pass my driving test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459800</th>\n",
       "      <td>4</td>\n",
       "      <td>2063786886</td>\n",
       "      <td>Sun Jun 07 04:04:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>zebslc</td>\n",
       "      <td>@SarahMillican75 you make yourself up to look ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839791</th>\n",
       "      <td>4</td>\n",
       "      <td>1559517560</td>\n",
       "      <td>Sun Apr 19 11:20:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>creena26</td>\n",
       "      <td>getting ready to go to the beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204046</th>\n",
       "      <td>4</td>\n",
       "      <td>1986191693</td>\n",
       "      <td>Sun May 31 18:28:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>none01</td>\n",
       "      <td>watching the MTV movie Awards, it's 3 am in Ge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0           1                             2         3  \\\n",
       "968114   4  1827836862  Sun May 17 11:32:58 PDT 2009  NO_QUERY   \n",
       "71229    0  1693896470  Mon May 04 00:12:28 PDT 2009  NO_QUERY   \n",
       "646698   0  2236575372  Fri Jun 19 04:02:46 PDT 2009  NO_QUERY   \n",
       "1011093  4  1881066754  Fri May 22 03:29:17 PDT 2009  NO_QUERY   \n",
       "321808   0  2003928077  Tue Jun 02 07:23:04 PDT 2009  NO_QUERY   \n",
       "1201765  4  1985734317  Sun May 31 17:40:19 PDT 2009  NO_QUERY   \n",
       "1345043  4  2043910645  Fri Jun 05 08:35:38 PDT 2009  NO_QUERY   \n",
       "1459800  4  2063786886  Sun Jun 07 04:04:19 PDT 2009  NO_QUERY   \n",
       "839791   4  1559517560  Sun Apr 19 11:20:57 PDT 2009  NO_QUERY   \n",
       "1204046  4  1986191693  Sun May 31 18:28:04 PDT 2009  NO_QUERY   \n",
       "\n",
       "                       4                                                  5  \n",
       "968114            uk_cam  @Claire_Cordon gender is just one way that lan...  \n",
       "71229          caaaaaity  missed the train , that tends to happen to me ...  \n",
       "646698   littleemilyjane  off school sick  mum refused to let me go to s...  \n",
       "1011093          ClareOz    whaat a loaad a rubbish  @mpisthename howsit? x  \n",
       "321808     LolaOnTwatter  i wish i could eat a huge bowl of capn crunch ...  \n",
       "1201765    wisdompathart  @Marge_Inovera awww. sending you some hugs and...  \n",
       "1345043     vickyjones91     i hate buses. so need to pass my driving test   \n",
       "1459800           zebslc  @SarahMillican75 you make yourself up to look ...  \n",
       "839791          creena26                  getting ready to go to the beach   \n",
       "1204046           none01  watching the MTV movie Awards, it's 3 am in Ge...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "sentiment_data = pd.read_csv('/Users/mj/Desktop/DSCI 314 (Text Mining)/training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', header = None)\n",
    "# Reducing dataframe size\n",
    "sentiment_data = sentiment_data.sample(frac=0.01)\n",
    "sentiment_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a564c614",
   "metadata": {},
   "source": [
    "### 2. Clean and preprocess the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97cc918d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 16000 entries, 968114 to 1144664\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       16000 non-null  int64 \n",
      " 1   3       16000 non-null  object\n",
      " 2   5       16000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 500.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Selecting features\n",
    "sentiment_data = sentiment_data.iloc[:, [0, 3, 5]]\n",
    "sentiment_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e19b36df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 16000 entries, 968114 to 1144664\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       16000 non-null  int64 \n",
      " 1   3       16000 non-null  object\n",
      " 2   5       16000 non-null  object\n",
      " 3   6       16000 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 625.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Creating columns\n",
    "sentiment_cols = [0, 3, 5]\n",
    "# Adding new column 6 which is polarity forecast\n",
    "sentiment_data[6] = sentiment_data[sentiment_cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "sentiment_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "23146d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 16000 entries, 968114 to 1144664\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       16000 non-null  int64 \n",
      " 1   6       16000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 375.0+ KB\n"
     ]
    }
   ],
   "source": [
    "sentiment_data = sentiment_data.iloc[:, [0, 3]]\n",
    "sentiment_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab5149a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "6    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_missing = sentiment_data.isnull().sum() * 100 / len(sentiment_data)\n",
    "percent_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa768f00",
   "metadata": {},
   "source": [
    "### 3. Build the first model based on pipeline using the support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6ca8bb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "1    0.502375\n",
       "0    0.497625\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding labels into numerical values\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    " \n",
    "sentiment_data[0] = le.fit_transform(sentiment_data.iloc[:, [0]].values.ravel())\n",
    "sentiment_data[0].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1b6dfa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dimension: (12800,); y_train dimension: (12800,)\n",
      "X_test dimension: (3200,); y_train dimension: (3200,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create features and label\n",
    "X = sentiment_data[0].astype(str)\n",
    "y = sentiment_data[6].astype(str)\n",
    "\n",
    "# Giving dimensions of test and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "print(f'X_train dimension: {X_train.shape}; y_train dimension: {y_train.shape}')\n",
    "print(f'X_test dimension: {X_test.shape}; y_train dimension: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4ba6394a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaner\u001b[39m\u001b[38;5;124m\"\u001b[39m, features()),\n\u001b[1;32m     13\u001b[0m                     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, tfidf_vector),\n\u001b[1;32m     14\u001b[0m                     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, classifier)])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# fit the model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:654\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    649\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    651\u001b[0m     )\n\u001b[1;32m    653\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 654\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params, raw_params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:588\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m    582\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    583\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[1;32m    584\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    585\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[1;32m    586\u001b[0m )\n\u001b[0;32m--> 588\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    589\u001b[0m     cloned_transformer,\n\u001b[1;32m    590\u001b[0m     X,\n\u001b[1;32m    591\u001b[0m     y,\n\u001b[1;32m    592\u001b[0m     weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    593\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    594\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[1;32m    595\u001b[0m     params\u001b[38;5;241m=\u001b[39mstep_params,\n\u001b[1;32m    596\u001b[0m )\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:1551\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1551\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[1;32m   1552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1554\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1555\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2103\u001b[0m )\n\u001b[0;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1373\u001b[0m             )\n\u001b[1;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1282\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1284\u001b[0m         )\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "\n",
    "# override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
    "tfidf_vector = TfidfVectorizer(ngram_range = (1,1))\n",
    "\n",
    "# support vector machine classifier\n",
    "classifier = svm.SVC()\n",
    "\n",
    "# create pipeline based on the cleaner, vectorizer, and classifier\n",
    "pipeline = Pipeline([(\"cleaner\", features()),\n",
    "                    (\"vectorizer\", tfidf_vector),\n",
    "                    (\"classifier\", classifier)])\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6716dd7",
   "metadata": {},
   "source": [
    "### 4. Check the first model. Is it a good model based on the selected evaluation metrics? Please justify your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cc98a3f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Pipeline is not fitted yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:54\u001b[0m, in \u001b[0;36m_raise_or_warn_if_not_fitted\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NotFittedError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:787\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 787\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \n\u001b[1;32m   2113\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2125\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2126\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2128\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1757\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Predict the test data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Print out the report\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred, target_names \u001b[38;5;241m=\u001b[39m target_names))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:782\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform the data, and apply `predict` with the final estimator.\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \n\u001b[1;32m    742\u001b[0m \u001b[38;5;124;03mCall `transform` of each transformer in the pipeline. The transformed\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;124;03m    Result of calling `predict` on the final estimator.\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# TODO(1.8): Remove the context manager and use check_is_fitted(self)\u001b[39;00m\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _raise_or_warn_if_not_fitted(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    783\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    153\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:56\u001b[0m, in \u001b[0;36m_raise_or_warn_if_not_fitted\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NotFittedError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline is not fitted yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# we only get here if the above didn't raise\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Pipeline is not fitted yet."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Specify target names since the y is label encodes using 0, 1 and 2\n",
    "target_names = ['negative', 'neutral', 'positive']\n",
    "# Predict the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "# Print out the report\n",
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52004123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
