{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf9efe9",
   "metadata": {},
   "source": [
    "### 1. Load the dataset of Twitter_Data.csv into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2100168f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kiya tho refresh maarkefir comment karo</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>surat women perform yagna seeks divine grace f...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this comes from cabinet which has scholars lik...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>with upcoming election india saga going import...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gandhi was gay does modi</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0\n",
       "5           kiya tho refresh maarkefir comment karo        0.0\n",
       "6  surat women perform yagna seeks divine grace f...       0.0\n",
       "7  this comes from cabinet which has scholars lik...       0.0\n",
       "8  with upcoming election india saga going import...       1.0\n",
       "9                         gandhi was gay does modi         1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "twitter_data = pd.read_csv('/Users/mj/Desktop/DSCI 314 (Text Mining)/Project4Data/Twitter_Data.csv', sep = ',')\n",
    "twitter_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e21db7",
   "metadata": {},
   "source": [
    "### 2. Convert the column of the clean_text to a matrix of token counts using CountVectorizer and unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f35d18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the feature matrix for the texts = (162980, 1199726)\n",
      "The first row of the feature matrix =   (0, 1145440)\t1\n",
      "  (0, 666553)\t1\n",
      "  (0, 831879)\t1\n",
      "  (0, 658439)\t1\n",
      "  (0, 435499)\t1\n",
      "  (0, 644088)\t1\n",
      "  (0, 435147)\t1\n",
      "  (0, 357405)\t1\n",
      "  (0, 480830)\t1\n",
      "  (0, 134189)\t1\n",
      "  (0, 1029272)\t2\n",
      "  (0, 299531)\t1\n",
      "  (0, 554530)\t1\n",
      "  (0, 867040)\t1\n",
      "  (0, 976966)\t2\n",
      "  (0, 1155022)\t1\n",
      "  (0, 308537)\t1\n",
      "  (0, 1006650)\t1\n",
      "  (0, 1183134)\t1\n",
      "  (0, 419834)\t1\n",
      "  (0, 562994)\t1\n",
      "  (0, 940186)\t2\n",
      "  (0, 66073)\t3\n",
      "  (0, 728515)\t1\n",
      "  (0, 175799)\t1\n",
      "  :\t:\n",
      "  (0, 357481)\t1\n",
      "  (0, 481028)\t1\n",
      "  (0, 134240)\t1\n",
      "  (0, 1032237)\t1\n",
      "  (0, 299593)\t1\n",
      "  (0, 555045)\t1\n",
      "  (0, 867045)\t1\n",
      "  (0, 1038737)\t1\n",
      "  (0, 977642)\t1\n",
      "  (0, 1155443)\t1\n",
      "  (0, 309052)\t1\n",
      "  (0, 1007448)\t1\n",
      "  (0, 1183662)\t1\n",
      "  (0, 420543)\t1\n",
      "  (0, 563133)\t1\n",
      "  (0, 977518)\t1\n",
      "  (0, 940260)\t1\n",
      "  (0, 72487)\t1\n",
      "  (0, 729194)\t1\n",
      "  (0, 175815)\t1\n",
      "  (0, 74556)\t1\n",
      "  (0, 940632)\t1\n",
      "  (0, 356855)\t1\n",
      "  (0, 838915)\t1\n",
      "  (0, 75386)\t1.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create vectorizer object\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Convert twitter_data csv row clean_text to matric of token counts\n",
    "matrix = vectorizer.fit_transform(twitter_data['clean_text'].values.astype('U'))\n",
    "\n",
    "# Summarizing numerical features from twitter_data matrix to show vectorizer worked\n",
    "print(f'The size of the feature matrix for the texts = {matrix.get_shape()}')\n",
    "print(f'The first row of the feature matrix = {matrix[0, ]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6643a8a4",
   "metadata": {},
   "source": [
    "### 3. Perform the tf-idf analasys on the column of the clean_text using CountVectorizer and TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7188d07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8389)\t0.1858717788592678\n",
      "  (0, 13684)\t0.2282834313221842\n",
      "  (0, 17907)\t0.18097741396817904\n",
      "  (0, 29341)\t0.20379916078250251\n",
      "  (0, 30477)\t0.1452398919187344\n",
      "  (0, 34636)\t0.2517650765565202\n",
      "  (0, 34701)\t0.20196161088022185\n",
      "  (0, 39396)\t0.12597803421480075\n",
      "  (0, 40499)\t0.1921722923035772\n",
      "  (0, 40527)\t0.12554675089481607\n",
      "  (0, 43980)\t0.11613399134304461\n",
      "  (0, 51357)\t0.15495537757250963\n",
      "  (0, 51985)\t0.2030659320709923\n",
      "  (0, 60317)\t0.21686927892530608\n",
      "  (0, 61637)\t0.1889103368727531\n",
      "  (0, 62481)\t0.03336926835844093\n",
      "  (0, 67998)\t0.08143445561404204\n",
      "  (0, 76937)\t0.1661604402232868\n",
      "  (0, 77543)\t0.2644440348083726\n",
      "  (0, 80438)\t0.3147731549491533\n",
      "  (0, 87792)\t0.2387429732629813\n",
      "  (0, 91104)\t0.3105076608504044\n",
      "  (0, 93828)\t0.13399957155223954\n",
      "  (0, 94774)\t0.23660466349027023\n",
      "  (0, 95482)\t0.1103369330198538\n",
      "  :\t:\n",
      "  (162979, 34101)\t0.12109384868662272\n",
      "  (162979, 34124)\t0.16072203830846327\n",
      "  (162979, 37266)\t0.06756510559574484\n",
      "  (162979, 41683)\t0.34070640025833954\n",
      "  (162979, 43151)\t0.09222016909436379\n",
      "  (162979, 44215)\t0.22911244118600835\n",
      "  (162979, 47231)\t0.12161833615006475\n",
      "  (162979, 56603)\t0.10462706020729617\n",
      "  (162979, 56816)\t0.1885267524368326\n",
      "  (162979, 58706)\t0.2631908704622821\n",
      "  (162979, 58707)\t0.27206877958543835\n",
      "  (162979, 62481)\t0.033547376352512766\n",
      "  (162979, 63947)\t0.11821826302011915\n",
      "  (162979, 65873)\t0.10545092368402069\n",
      "  (162979, 69383)\t0.10561082092724193\n",
      "  (162979, 74812)\t0.15343395605175808\n",
      "  (162979, 77339)\t0.15060846405684516\n",
      "  (162979, 82981)\t0.34251537535586357\n",
      "  (162979, 89693)\t0.17731376066639348\n",
      "  (162979, 93828)\t0.13471479235478426\n",
      "  (162979, 95375)\t0.08325502496936818\n",
      "  (162979, 95482)\t0.05546292741331127\n",
      "  (162979, 103788)\t0.13830510254989895\n",
      "  (162979, 104111)\t0.08352155226743457\n",
      "  (162979, 105957)\t0.08074397921480718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Create a new vectorizer object\n",
    "vectorizer1 = CountVectorizer()\n",
    "\n",
    "# Extract token counts from twitter_data row clean_text\n",
    "token_count = vectorizer1.fit_transform(twitter_data['clean_text'].values.astype('U'))\n",
    "\n",
    "# Create a tf_idf object\n",
    "tfidftran = TfidfTransformer(use_idf = True, smooth_idf = True, sublinear_tf = False) \n",
    "\n",
    "# Fit to the count matrix then transform\n",
    "tfidfmatrix = tfidftran.fit_transform(token_count)\n",
    "print(tfidfmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e850e5",
   "metadata": {},
   "source": [
    "### 4. Perform the tf-idf analysis on the column of the clean_text using Tfidfvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3e6d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 103780)\t0.11668604692600501\n",
      "  (0, 62481)\t0.03336926835844093\n",
      "  (0, 76937)\t0.1661604402232868\n",
      "  (0, 61637)\t0.1889103368727531\n",
      "  (0, 40527)\t0.12554675089481607\n",
      "  (0, 60317)\t0.21686927892530608\n",
      "  (0, 40499)\t0.1921722923035772\n",
      "  (0, 34701)\t0.20196161088022185\n",
      "  (0, 43980)\t0.11613399134304461\n",
      "  (0, 13684)\t0.2282834313221842\n",
      "  (0, 95482)\t0.1103369330198538\n",
      "  (0, 29341)\t0.20379916078250251\n",
      "  (0, 51357)\t0.15495537757250963\n",
      "  (0, 80438)\t0.3147731549491533\n",
      "  (0, 91104)\t0.3105076608504044\n",
      "  (0, 103994)\t0.10858429770134567\n",
      "  (0, 30477)\t0.1452398919187344\n",
      "  (0, 93828)\t0.13399957155223954\n",
      "  (0, 105521)\t0.12028353756840601\n",
      "  (0, 39396)\t0.12597803421480075\n",
      "  (0, 51985)\t0.2030659320709923\n",
      "  (0, 87792)\t0.2387429732629813\n",
      "  (0, 8389)\t0.1858717788592678\n",
      "  (0, 67998)\t0.08143445561404204\n",
      "  (0, 17907)\t0.18097741396817904\n",
      "  :\t:\n",
      "  (162979, 65873)\t0.10545092368402072\n",
      "  (162979, 56603)\t0.10462706020729619\n",
      "  (162979, 95375)\t0.0832550249693682\n",
      "  (162979, 63947)\t0.11821826302011917\n",
      "  (162979, 5841)\t0.1755040283953335\n",
      "  (162979, 5191)\t0.11241623159748944\n",
      "  (162979, 74812)\t0.15343395605175814\n",
      "  (162979, 17962)\t0.0930068253672693\n",
      "  (162979, 69383)\t0.10561082092724194\n",
      "  (162979, 43151)\t0.0922201690943638\n",
      "  (162979, 103788)\t0.13830510254989897\n",
      "  (162979, 47231)\t0.12161833615006477\n",
      "  (162979, 82981)\t0.3425153753558636\n",
      "  (162979, 34101)\t0.12109384868662275\n",
      "  (162979, 34124)\t0.1607220383084633\n",
      "  (162979, 89693)\t0.1773137606663935\n",
      "  (162979, 77339)\t0.1506084640568452\n",
      "  (162979, 10864)\t0.3364426864094962\n",
      "  (162979, 44215)\t0.22911244118600838\n",
      "  (162979, 25873)\t0.20296190068031533\n",
      "  (162979, 56816)\t0.18852675243683265\n",
      "  (162979, 58707)\t0.2720687795854384\n",
      "  (162979, 29767)\t0.25797207796634797\n",
      "  (162979, 58706)\t0.2631908704622822\n",
      "  (162979, 41683)\t0.3407064002583396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer object using default parameters\n",
    "tfidfvector = TfidfVectorizer(use_idf = True, smooth_idf = True, sublinear_tf = False)\n",
    "\n",
    "# Fit to the corpus then convert twitter_data row clean_text to a matrix\n",
    "tfidfmatrix2 = tfidfvector.fit_transform(twitter_data['clean_text'].values.astype('U'))\n",
    "print(tfidfmatrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90160dae",
   "metadata": {},
   "source": [
    "### 5. Perform the tf-idf analysis on the column of the clean_text using HashingVectorizer and TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a361fda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 160541)\t0.19218286869297355\n",
      "  (0, 168557)\t0.314790478796511\n",
      "  (0, 180525)\t-0.1858820084937663\n",
      "  (0, 232512)\t0.11669246885631876\n",
      "  (0, 263274)\t0.2167508998548927\n",
      "  (0, 277794)\t-0.25177893269287643\n",
      "  (0, 286878)\t-0.11034300552049593\n",
      "  (0, 288398)\t0.22829599512331\n",
      "  (0, 360502)\t0.3104947539357193\n",
      "  (0, 387101)\t-0.1889207337372532\n",
      "  (0, 433698)\t0.03337110486864643\n",
      "  (0, 434864)\t0.20307710800247009\n",
      "  (0, 449993)\t-0.1340069463492547\n",
      "  (0, 465141)\t-0.16616958502505294\n",
      "  (0, 482215)\t-0.14524788533773567\n",
      "  (0, 484920)\t-0.12029015749012335\n",
      "  (0, 490370)\t0.23875611271519162\n",
      "  (0, 522187)\t0.18098737423653574\n",
      "  (0, 614924)\t0.15496390569273244\n",
      "  (0, 646934)\t0.10859027374399496\n",
      "  (0, 747378)\t-0.2366176852585182\n",
      "  (0, 748718)\t0.12555366048260833\n",
      "  (0, 808196)\t-0.11614038289044132\n",
      "  (0, 839641)\t-0.26445858874355305\n",
      "  (0, 865698)\t0.12597960273256398\n",
      "  :\t:\n",
      "  (162979, 257965)\t0.18852715420744698\n",
      "  (162979, 286878)\t-0.05546304561072428\n",
      "  (162979, 338809)\t0.11240663186544564\n",
      "  (162979, 372702)\t0.09300702357473262\n",
      "  (162979, 408714)\t-0.11821449781008107\n",
      "  (162979, 413699)\t-0.2720693593930217\n",
      "  (162979, 433698)\t0.03354744784554934\n",
      "  (162979, 449993)\t-0.13471507944637306\n",
      "  (162979, 487855)\t-0.08325520239470491\n",
      "  (162979, 507870)\t-0.2291129294490536\n",
      "  (162979, 512176)\t0.08443263143820844\n",
      "  (162979, 528700)\t-0.10561104599516391\n",
      "  (162979, 642085)\t0.17550440241313411\n",
      "  (162979, 675997)\t0.0835217302607694\n",
      "  (162979, 707819)\t0.3407071263400122\n",
      "  (162979, 730607)\t-0.1054511484111846\n",
      "  (162979, 731192)\t0.3364434034047399\n",
      "  (162979, 800174)\t-0.25797262773235685\n",
      "  (162979, 814105)\t0.16072238082426668\n",
      "  (162979, 832412)\t0.08074415128883762\n",
      "  (162979, 865514)\t0.13830539729282165\n",
      "  (162979, 865966)\t0.153434283035881\n",
      "  (162979, 975831)\t-0.10462728317871917\n",
      "  (162979, 994433)\t0.09222036562537869\n",
      "  (162979, 1031365)\t0.1506087850195433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Create a hash vectorizer object using default parameters\n",
    "hash_vectorizer = HashingVectorizer()\n",
    "\n",
    "# Convert twitter_data row clean_text to a matrix of token counts\n",
    "token_count3 = hash_vectorizer.fit_transform(twitter_data['clean_text'].values.astype('U'))\n",
    "\n",
    "# Reusing previously created a tf_idf object\n",
    "# Fit to the count matrix, then transform it to a normalized tf-idf representation\n",
    "tfidfmatrix3 = tfidftran.fit_transform(token_count3)\n",
    "\n",
    "print(tfidfmatrix3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52bea9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
